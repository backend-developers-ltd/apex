{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-31 11:32:32,311\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:201: UserWarning: Field name \"required_hash_fields\" in \"StreamPromptingSynapse\" shadows an attribute in parent \"StreamingSynapse\"\n",
      "  warnings.warn(\n",
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_type\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from prompting.tasks.qa import QuestionAnsweringTask, QARewardConfig\n",
    "# from prompting.tasks.mock import MockTask, MockRewardConfig\n",
    "from prompting.tasks.summarization import SummarizationTask, SummarizationRewardConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bittensor: - Available free memory: 44.65 GB - \n",
      "INFO:bittensor: - Total gpu memory 47.62 GB - \n",
      "INFO:bittensor: - 45.0% of the GPU memory will be utilized for loading the model to device \"CUDA\". - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-31 11:32:36.355 |       INFO       |  - Available free memory: 44.65 GB - \n",
      "2024-07-31 11:32:36.359 |       INFO       |  - Total gpu memory 47.62 GB - \n",
      "2024-07-31 11:32:36.362 |       INFO       |  - 45.0% of the GPU memory will be utilized for loading the model to device \"CUDA\". - \n",
      "2024-07-31 11:32:59.729 |       INFO       |  - 🤖 Generating query... - \n",
      "2024-07-31 11:33:00.124 |       INFO       |  - vLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "Can you tell me more about Josh Banks? Who is he, and what's his story? What makes him interesting or noteworthy? - \n",
      "2024-07-31 11:33:00.127 |       INFO       |  - 🤖 Generating reference... - \n",
      "2024-07-31 11:33:00.403 |       INFO       |  - vLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "The provided context is related to the topic of \"Career\". - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-31 11:32:36 config.py:244] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 07-31 11:32:36 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='casperhansen/llama-3-8b-instruct-awq', speculative_config=None, tokenizer='casperhansen/llama-3-8b-instruct-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/llama-3-8b-instruct-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 07-31 11:32:37 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 07-31 11:32:39 model_runner.py:255] Loading model weights took 5.3440 GB\n",
      "INFO 07-31 11:32:41 gpu_executor.py:84] # GPU blocks: 6573, # CPU blocks: 2048\n",
      "INFO 07-31 11:32:45 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-31 11:32:45 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-31 11:32:59 model_runner.py:1117] Graph capturing finished in 15 secs.\n"
     ]
    }
   ],
   "source": [
    "# Initialise the LLM we use on the validator\n",
    "from prompting.llms.vllm_llm import vLLMPipeline\n",
    "pipeline = vLLMPipeline(llm_model_id=\"casperhansen/llama-3-8b-instruct-awq\", llm_max_allowed_memory_in_gb=20, device=\"CUDA\", quantization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.26s/it, est. speed input: 2.15 toks/s, output: 78.47 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"))  # -> \"What\\'s 1+2?\"\\nprint(eval(\"1+2\"))  # -> 3\\nprint(eval(\"True and False\"))  # -> False\\nprint(eval(\"True and (True or False)\"))  # -> True\\n```\\n\\nIn Python, `eval()` is a built-in function that parses the expression passed to this method and executes Python expression(s) passed as a string.\\n\\nHere are some important things to note about `eval()`:\\n\\n1.  `eval()` evaluates the expression in the current scope. If you want to evaluate an expression in a different scope, you need to use the `exec()` function instead.\\n2.  `eval()` can execute arbitrary Python code. This makes it possible to execute any Python code as a string, but it also makes it a potential security risk, especially when dealing with untrusted input.\\n3.  `eval()` is generally considered unsafe for use with untrusted input, such as user-supplied data, because it can execute any Python code. In particular, if the input contains a string that is used as an argument to `os.system()`, `subprocess.Popen()`, or `os.popen()`, it can be used to execute arbitrary system commands.\\n4. '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you can see, \"pipeline\" is an object that simply wraps around the LLM and is callable\n",
    "pipeline(\"What's 1+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Datasets generate 'Context' objects, which contain a 'row' of data, in this case about wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:201: UserWarning: Field name \"name\" in \"WikiDateDataset\" shadows an attribute in parent \"BaseDataset\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Context(title='Josh Banks', topic='Career', subtopic='Career', content='Career', internal_links=['Career', 'Personal life'], external_links=['2010 in baseball', 'Baseball', 'Major League Baseball', '2007 Major League Baseball season', '2010 Major League Baseball season', '2008 in baseball', 'Florida International University', 'San Diego Padres', 'Toronto Blue Jays', '2003 Major League Baseball Draft'], source='Wikipedia', tags=['1982 births', 'American expatriate baseball players in Canada', 'Auburn Doubledays players', 'Baseball players from Anne Arundel County, Maryland', 'Baseball players from Baltimore', 'Cotuit Kettleers players', 'Dunedin Blue Jays players', 'FIU Panthers baseball players', 'Fresno Grizzlies players', 'Houston Astros players', 'Living people', 'Long Island Ducks players', 'Major League Baseball pitchers', 'New Hampshire Fisher Cats players', 'People from Severna Park, Maryland', 'Portland Beavers players', 'Round Rock Express players', 'San Diego Padres players', 'Short description is different from Wikidata', 'Syracuse Chiefs players', 'Syracuse SkyChiefs players', 'Toronto Blue Jays players', 'Use mdy dates from July 2024'], extra={'url': 'https://en.wikipedia.org/wiki/Josh_Banks', 'page_length': 376, 'section_length': 267}, stats=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompting.tools.datasets import WikiDataset\n",
    "dataset = WikiDataset()\n",
    "context = dataset.random()\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Tasks are objects that can be used to generate the query & reference for a miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise with past data\n",
    "\n",
    "We can either initialise the task with past data (this doesn't require an LLM to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SummarizationTask(query=\"Summarize the following text: 'The quick brown fox jumps over the lazy dog.'\", reference=\"A fox jumps over a dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new task\n",
    "\n",
    "Or we create a new task - this requires using the LLM pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bittensor: - 🤖 Generating query... - \n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s, est. speed input: 84.72 toks/s, output: 74.13 toks/s]\n",
      "INFO:bittensor: - vLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "Can you tell me more about Josh Banks? Who is he, and what's his story? What makes him interesting or noteworthy? - \n",
      "INFO:bittensor: - 🤖 Generating reference... - \n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s, est. speed input: 883.35 toks/s, output: 53.53 toks/s]\n",
      "INFO:bittensor: - vLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "The provided context is related to the topic of \"Career\". - \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe provided context is related to the topic of \"Career\".'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = SummarizationTask(context=context, static_query=False)\n",
    "task.generate_query(pipeline=pipeline)\n",
    "task.generate_reference(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miner Responses\n",
    "\n",
    "Now let's say we have a few miners giving us responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompting.dendrite import DendriteResponseEvent, SynapseStreamResult, StreamPromptingSynapse\n",
    "\n",
    "miner_response_1 = SynapseStreamResult(synapse=StreamPromptingSynapse(completion=\"Hey, how are you?\", roles=[\"assistant\"], messages=[\"What's up with you?\"]))\n",
    "miner_response_2 = SynapseStreamResult(synapse=StreamPromptingSynapse(completion=\"How are you doing, mate?\", roles=[\"assistant\"], messages=[\"What's up with you?\"]))\n",
    "\n",
    "\n",
    "# the synapses from all miners get collected into the DenriteResponseEvent\n",
    "dendrite_response = DendriteResponseEvent(stream_results=[miner_response_1, miner_response_2], uids=[1, 2], timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6115211, 0.4293716], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward = SummarizationRewardConfig()\n",
    "reward.apply(reference = \"Hey, how are you?\", query=\"Hey, how are you?\", response_event=dendrite_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompting.rewards.rouge import RougeRewardModel\n",
    "\n",
    "rouge = RougeRewardModel()\n",
    "rev  = rouge.apply(reference = \"Hey, how are you?\", response_event=dendrite_response, reward_type=\"reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.2222])\n",
      "tensor([0.0000, 0.2222])\n",
      "tensor([0.0000, 0.2222])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.       , 0.6666667], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "revs = [rev] * 3\n",
    "\n",
    "for rev in revs:\n",
    "    print(rev.rewards)\n",
    "    \n",
    "np.sum([rev.rewards for rev in revs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4722)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward = QARewardConfig()\n",
    "\n",
    "reward.apply(reference = \"Hey, how are you?\", response_event=dendrite_response)\n",
    "reward.final_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:201: UserWarning: Field name \"name\" in \"WikiDateDataset\" shadows an attribute in parent \"BaseDataset\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Context(title='Sankhuwasabha District', topic='Demographics', subtopic='Demographics', content='Demographics', internal_links=['Geography and climate', 'Demographics', 'Divisions'], external_links=['Koshi Province', 'Solukhumbu', 'Solukhumbu District', 'Taplejung District', 'Achham District', 'Arghakhanchi District', 'Baglung District', 'Bagmati Province', 'Baitadi District', 'Bajhang District'], source='Wikipedia', tags=['Commons category link from Wikidata', 'Coordinates on Wikidata', 'Districts of Koshi Province', 'Districts of Nepal established in 1962', 'Koshi Province geography stubs', 'Sankhuwasabha District', 'Short description is different from Wikidata'], extra={'url': 'https://en.wikipedia.org/wiki/Sankhuwasabha_District', 'page_length': 336, 'section_length': 178}, stats=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-31 09:59:53 config.py:244] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 07-31 09:59:53 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='casperhansen/llama-3-8b-instruct-awq', speculative_config=None, tokenizer='casperhansen/llama-3-8b-instruct-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/llama-3-8b-instruct-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 07-31 09:59:56 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 07-31 10:00:20 model_runner.py:255] Loading model weights took 5.3440 GB\n",
      "INFO 07-31 10:00:22 gpu_executor.py:84] # GPU blocks: 5890, # CPU blocks: 2048\n",
      "INFO 07-31 10:00:26 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-31 10:00:26 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-31 10:00:37 model_runner.py:1117] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 qa = SummarizationTask(context=context, llm_pipeline=pipeline, reward_config=Summarizati     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'SummarizationTask'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 qa = SummarizationTask(context=context, llm_pipeline=pipeline, reward_config=Summarizati     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'SummarizationTask'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa = SummarizationTask(context=context, llm_pipeline=pipeline, reward_config=SummarizationRewardConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 139.69 toks/s, output: 343.83 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 567.92 toks/s, output: 360.75 toks/s]\n"
     ]
    }
   ],
   "source": [
    "qa_task = QuestionAnsweringTask(llm_pipeline=pipeline, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{{ Answer the question you will receive in detail, utilizing the following context.\n",
      "\n",
      "# Context:\n",
      "\n",
      "History\n",
      "\n",
      "# Question:\n",
      "\n",
      "None\n",
      "\n",
      " }}<|eot_id|><|start_header_id|>presenter<|end_header_id|>\n",
      "\n",
      "{{ Answer the question you will receive in detail, utilizing the following context.\n",
      "\n",
      "# Context:\n",
      "\n",
      "History\n",
      "\n",
      "# Question:\n",
      "\n",
      "None\n",
      "\n",
      " }}<|eot_id|><|start_header_id|>active<|end_header_id|>\n",
      "\n",
      "{{ Answer the question you will receive in detail, utilizing the following context.\n",
      "\n",
      "# Context:\n",
      "\n",
      "History\n",
      "\n",
      "# Question:\n",
      "\n",
      "None\n",
      "\n",
      " }}<|eot_id|><|start_header_id|>active<|end_header_id|>\n",
      "\n",
      "{{ Answer the question you will receive in detail, utilizing the following context.\n",
      "\n",
      "# Context:\n",
      "\n",
      "History\n",
      "\n",
      "# Question:\n",
      "\n",
      "None\n",
      "\n",
      " }}<|eot_id|><|start_header_id|>active<|end_header_id|>\n",
      "\n",
      "{{ Answer the question you\n"
     ]
    }
   ],
   "source": [
    "print(qa_task.reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskConfig(task='Hello', probability=0.9, datasets=['WikiDataset'])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import ClassVar\n",
    "import random \n",
    "\n",
    "class TaskConfig(BaseModel):\n",
    "    task: str\n",
    "    probability: float\n",
    "    datasets: list[str]\n",
    "\n",
    "\n",
    "class TaskRegistry(BaseModel):\n",
    "    tasks: ClassVar[list[TaskConfig]] = [\n",
    "        TaskConfig(task=\"Hello\", probability=0.9, datasets=[\"WikiDataset\"]),\n",
    "        TaskConfig(task=\"Hello1\", probability=0.4, datasets=[\"WikiDataset\"]),\n",
    "        # TaskConfig(task=DateQuestionAnsweringTask, probability=0.2, datasets=[WikiDateDataset])\n",
    "    ]\n",
    "\n",
    "    @classmethod\n",
    "    def random(cls):\n",
    "        probabilities = [task.probability for task in cls.tasks]\n",
    "        selected_task = random.choices(cls.tasks, probabilities)[0]\n",
    "        return selected_task\n",
    "    \n",
    "TaskRegistry.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaskConfig(), TaskConfig()]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaskRegistry.tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompting-fb5sw-i7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
