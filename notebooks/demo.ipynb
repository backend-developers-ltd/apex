{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-31 11:32:32,311\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:201: UserWarning: Field name \"required_hash_fields\" in \"StreamPromptingSynapse\" shadows an attribute in parent \"StreamingSynapse\"\n",
      "  warnings.warn(\n",
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_type\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from prompting.tasks.qa import QuestionAnsweringTask, QARewardConfig\n",
    "# from prompting.tasks.mock import MockTask, MockRewardConfig\n",
    "from prompting.tasks.summarization import SummarizationTask, SummarizationRewardConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bittensor: - Available free memory: 44.65 GB - \n",
      "INFO:bittensor: - Total gpu memory 47.62 GB - \n",
      "INFO:bittensor: - 45.0% of the GPU memory will be utilized for loading the model to device \"CUDA\". - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-31 11:32:36.355 |       INFO       |  - Available free memory: 44.65 GB - \n",
      "2024-07-31 11:32:36.359 |       INFO       |  - Total gpu memory 47.62 GB - \n",
      "2024-07-31 11:32:36.362 |       INFO       |  - 45.0% of the GPU memory will be utilized for loading the model to device \"CUDA\". - \n",
      "2024-07-31 11:32:59.729 |       INFO       |  - ðŸ¤– Generating query... - \n",
      "2024-07-31 11:33:00.124 |       INFO       |  - vLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "Can you tell me more about Josh Banks? Who is he, and what's his story? What makes him interesting or noteworthy? - \n",
      "2024-07-31 11:33:00.127 |       INFO       |  - ðŸ¤– Generating reference... - \n",
      "2024-07-31 11:33:00.403 |       INFO       |  - vLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "The provided context is related to the topic of \"Career\". - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-31 11:32:36 config.py:244] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 07-31 11:32:36 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='casperhansen/llama-3-8b-instruct-awq', speculative_config=None, tokenizer='casperhansen/llama-3-8b-instruct-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/llama-3-8b-instruct-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 07-31 11:32:37 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 07-31 11:32:39 model_runner.py:255] Loading model weights took 5.3440 GB\n",
      "INFO 07-31 11:32:41 gpu_executor.py:84] # GPU blocks: 6573, # CPU blocks: 2048\n",
      "INFO 07-31 11:32:45 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-31 11:32:45 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-31 11:32:59 model_runner.py:1117] Graph capturing finished in 15 secs.\n"
     ]
    }
   ],
   "source": [
    "# Initialise the LLM we use on the validator\n",
    "from prompting.llms.vllm_llm import vLLMPipeline\n",
    "pipeline = vLLMPipeline(llm_model_id=\"casperhansen/llama-3-8b-instruct-awq\", llm_max_allowed_memory_in_gb=20, device=\"CUDA\", quantization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.26s/it, est. speed input: 2.15 toks/s, output: 78.47 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"))  # -> \"What\\'s 1+2?\"\\nprint(eval(\"1+2\"))  # -> 3\\nprint(eval(\"True and False\"))  # -> False\\nprint(eval(\"True and (True or False)\"))  # -> True\\n```\\n\\nIn Python, `eval()` is a built-in function that parses the expression passed to this method and executes Python expression(s) passed as a string.\\n\\nHere are some important things to note about `eval()`:\\n\\n1.  `eval()` evaluates the expression in the current scope. If you want to evaluate an expression in a different scope, you need to use the `exec()` function instead.\\n2.  `eval()` can execute arbitrary Python code. This makes it possible to execute any Python code as a string, but it also makes it a potential security risk, especially when dealing with untrusted input.\\n3.  `eval()` is generally considered unsafe for use with untrusted input, such as user-supplied data, because it can execute any Python code. In particular, if the input contains a string that is used as an argument to `os.system()`, `subprocess.Popen()`, or `os.popen()`, it can be used to execute arbitrary system commands.\\n4. '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you can see, \"pipeline\" is an object that simply wraps around the LLM and is callable\n",
    "pipeline(\"What's 1+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Datasets generate 'Context' objects, which contain a 'row' of data, in this case about wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:201: UserWarning: Field name \"name\" in \"WikiDateDataset\" shadows an attribute in parent \"BaseDataset\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Context(title='Josh Banks', topic='Career', subtopic='Career', content='Career', internal_links=['Career', 'Personal life'], external_links=['2010 in baseball', 'Baseball', 'Major League Baseball', '2007 Major League Baseball season', '2010 Major League Baseball season', '2008 in baseball', 'Florida International University', 'San Diego Padres', 'Toronto Blue Jays', '2003 Major League Baseball Draft'], source='Wikipedia', tags=['1982 births', 'American expatriate baseball players in Canada', 'Auburn Doubledays players', 'Baseball players from Anne Arundel County, Maryland', 'Baseball players from Baltimore', 'Cotuit Kettleers players', 'Dunedin Blue Jays players', 'FIU Panthers baseball players', 'Fresno Grizzlies players', 'Houston Astros players', 'Living people', 'Long Island Ducks players', 'Major League Baseball pitchers', 'New Hampshire Fisher Cats players', 'People from Severna Park, Maryland', 'Portland Beavers players', 'Round Rock Express players', 'San Diego Padres players', 'Short description is different from Wikidata', 'Syracuse Chiefs players', 'Syracuse SkyChiefs players', 'Toronto Blue Jays players', 'Use mdy dates from July 2024'], extra={'url': 'https://en.wikipedia.org/wiki/Josh_Banks', 'page_length': 376, 'section_length': 267}, stats=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompting.tools.datasets import WikiDataset\n",
    "dataset = WikiDataset()\n",
    "context = dataset.random()\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Tasks are objects that can be used to generate the query & reference for a miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise with past data\n",
    "\n",
    "We can either initialise the task with past data (this doesn't require an LLM to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SummarizationTask(query=\"Summarize the following text: 'The quick brown fox jumps over the lazy dog.'\", reference=\"A fox jumps over a dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new task\n",
    "\n",
    "Or we create a new task - this requires using the LLM pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bittensor: - ðŸ¤– Generating query... - \n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.63it/s, est. speed input: 84.72 toks/s, output: 74.13 toks/s]\n",
      "INFO:bittensor: - vLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "Can you tell me more about Josh Banks? Who is he, and what's his story? What makes him interesting or noteworthy? - \n",
      "INFO:bittensor: - ðŸ¤– Generating reference... - \n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.79it/s, est. speed input: 883.35 toks/s, output: 53.53 toks/s]\n",
      "INFO:bittensor: - vLLM_LLM generated the following output:\n",
      "\n",
      "\n",
      "The provided context is related to the topic of \"Career\". - \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe provided context is related to the topic of \"Career\".'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = SummarizationTask(context=context, static_query=False)\n",
    "task.generate_query(pipeline=pipeline)\n",
    "task.generate_reference(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miner Responses\n",
    "\n",
    "Now let's say we have a few miners giving us responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompting.dendrite import DendriteResponseEvent, SynapseStreamResult, StreamPromptingSynapse\n",
    "\n",
    "miner_response_1 = SynapseStreamResult(synapse=StreamPromptingSynapse(completion=\"4\", roles=[\"user\"], messages=[\"What's 1+2?\"]))\n",
    "miner_response_2 = SynapseStreamResult(synapse=StreamPromptingSynapse(completion=\"3\", roles=[\"assistant\"], messages=[\"What's 1+2?\"]))\n",
    "\n",
    "\n",
    "# the synapses from all miners get collected into the DenriteResponseEvent\n",
    "dendrite_response = DendriteResponseEvent(stream_results=[miner_response_1, miner_response_2], uids=[1, 2], timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "\n",
    "We can now pass the query, reference and miner responses to our scoring function, which is then responsible for giving each miner a score which is later used to set weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03413734, 0.17849946], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward = SummarizationRewardConfig()\n",
    "reward.apply(query=\"What's 1+2?\", reference=\"1+2 is equal to 3\", response_event=dendrite_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other tests/examples on different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompting.tasks.qa import QuestionAnsweringTask, QARewardConfig\n",
    "qa = QuestionAnsweringTask(context=context, llm_pipeline=pipeline, reward_config=SummarizationRewardConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.generate_query(pipeline=pipeline)\n",
    "qa.generate_reference(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 139.69 toks/s, output: 343.83 toks/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 567.92 toks/s, output: 360.75 toks/s]\n"
     ]
    }
   ],
   "source": [
    "qa_task = QuestionAnsweringTask(llm_pipeline=pipeline, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{{ Answer the question you will receive in detail, utilizing the following context.\n",
      "\n",
      "# Context:\n",
      "\n",
      "History\n",
      "\n",
      "# Question:\n",
      "\n",
      "None\n",
      "\n",
      " }}<|eot_id|><|start_header_id|>presenter<|end_header_id|>\n",
      "\n",
      "{{ Answer the question you will receive in detail, utilizing the following context.\n",
      "\n",
      "# Context:\n",
      "\n",
      "History\n",
      "\n",
      "# Question:\n",
      "\n",
      "None\n",
      "\n",
      " }}<|eot_id|><|start_header_id|>active<|end_header_id|>\n",
      "\n",
      "{{ Answer the question you will receive in detail, utilizing the following context.\n",
      "\n",
      "# Context:\n",
      "\n",
      "History\n",
      "\n",
      "# Question:\n",
      "\n",
      "None\n",
      "\n",
      " }}<|eot_id|><|start_header_id|>active<|end_header_id|>\n",
      "\n",
      "{{ Answer the question you will receive in detail, utilizing the following context.\n",
      "\n",
      "# Context:\n",
      "\n",
      "History\n",
      "\n",
      "# Question:\n",
      "\n",
      "None\n",
      "\n",
      " }}<|eot_id|><|start_header_id|>active<|end_header_id|>\n",
      "\n",
      "{{ Answer the question you\n"
     ]
    }
   ],
   "source": [
    "print(qa_task.reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaskConfig(), TaskConfig()]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaskRegistry.tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompting-fb5sw-i7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
