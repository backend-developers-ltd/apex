{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:20:24.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mPrompting version: 2.7.0\u001b[0m\n",
      "\u001b[32m2024-08-14 12:20:27.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.utils.config\u001b[0m:\u001b[36mconfig\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mRUNNING WITH ARGS: netuid=None wallet.name=None wallet.hotkey=None subtensor.network=None axon.port=None\u001b[0m\n",
      "\u001b[32m2024-08-14 12:20:27.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.settings\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mConfig: \n",
      "netuid: null\n",
      "wallet:\n",
      "  name: null\n",
      "  hotkey: null\n",
      "subtensor:\n",
      "  network: null\n",
      "axon:\n",
      "  port: null\n",
      "no_prompt: false\n",
      "config: null\n",
      "strict: false\n",
      "no_version_checking: false\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 12:20:27.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.utils.config\u001b[0m:\u001b[36mconfig\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mRUNNING WITH ARGS: netuid=None wallet.name=None wallet.hotkey=None subtensor.network=None axon.port=None\u001b[0m\n",
      "\u001b[32m2024-08-14 12:20:27.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.settings\u001b[0m:\u001b[36mload_env\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mInstantiating bittensor objects with NETUID: 61, WALLET_NAME: validator, HOTKEY: validator_hotkey\u001b[0m\n",
      "\u001b[32m2024-08-14 12:20:31.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprompting.settings\u001b[0m:\u001b[36mload_env\u001b[0m:\u001b[36m130\u001b[0m - \u001b[1mBittensor objects instantiated... WALLET: wallet(validator, validator_hotkey, ~/.bittensor/wallets/), SUBTENSOR: subtensor(test, wss://test.finney.opentensor.ai:443/), METAGRAPH: metagraph(netuid:61, n:225, block:2572042, network:test)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode='validator' MOCK=False NO_BACKGROUND_THREAD=True WANDB_ON=True WANDB_ENTITY='felix-quinque-macrocosmos-ai' WANDB_PROJECT_NAME='validator' WANDB_RUN_STEP_LENGTH=100 WANDB_API_KEY='ae29a588c238d0e168d620e0b18a5e29e283935a' WANDB_OFFLINE=False WANDB_NOTES='' SAVE_PATH='./storage' NEURON_EPOCH_LENGTH=1 NEURON_DEVICE='cuda' NEURON_GPUS=1 LOGGING_DONT_SAVE_EVENTS=False NEURON_TIMEOUT=15 NEURON_DISABLE_SET_WEIGHTS=False NEURON_MOVING_AVERAGE_ALPHA=0.1 NEURON_DECAY_ALPHA=0.001 NEURON_AXON_OFF=False NEURON_VPERMIT_TAO_LIMIT=4096 NEURON_QUERY_UNIQUE_COLDKEYS=False NEURON_QUERY_UNIQUE_IPS=False NEURON_FORWARD_MAX_TIME=120 ORGANIC_TIMEOUT=15 ORGANIC_SAMPLE_SIZE=10 ORGANIC_REUSE_RESPONSE_DISABLED=False ORGANIC_REFERENCE_MAX_TOKENS=256 ORGANIC_SYNTH_REWARD_SCALE=1.0 ORGANIC_SET_WEIGHTS_ENABLED=True ORGANIC_DISABLED=False ORGANIC_TRIGGER_FREQUENCY=120 ORGANIC_TRIGGER_FREQUENCY_MIN=5 ORGANIC_TRIGGER='seconds' ORGANIC_SCALING_FACTOR=1 LOG_FULL=False NETUID=61 TEST=True OPENAI_API_KEY='sk-proj-Cq96kZG43OwGM1u3lojpT3BlbkFJqdZJz93Uzre3Z3hdQMfw' WALLET_NAME='validator' HOTKEY='validator_hotkey' AXON_PORT=22118 ORGANIC_WHITELIST_HOTKEY='5F4tQyWrhfGVcNhoqeiNsR6KjD4wMZ2kfhLj4oHYuyHbZAc3' TEST_MINER_IDS=[193] SUBTENSOR_NETWORK='test' WALLET=wallet(validator, validator_hotkey, ~/.bittensor/wallets/) SUBTENSOR=subtensor(test, wss://test.finney.opentensor.ai:443/) METAGRAPH=metagraph(netuid:61, n:225, block:2572042, network:test) NEURON_LLM_MAX_ALLOWED_MEMORY_IN_GB=62 NEURON_MODEL_ID_VALIDATOR='casperhansen/llama-3-70b-instruct-awq'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(44.3516845703125, 44.089599609375, 0.2620849609375)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompting.llms.utils import GPUInfo\n",
    "GPUInfo.total_memory, GPUInfo.free_memory, GPUInfo.used_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-14 12:20:32,206\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_manager\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import ClassVar\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "from pydantic import BaseModel, ConfigDict, model_validator\n",
    "import torch\n",
    "import vllm\n",
    "\n",
    "\n",
    "import vllm\n",
    "import numpy as np\n",
    "from prompting.llms.utils import GPUInfo\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel\n",
    "from prompting.settings import settings\n",
    "\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    model_id: str\n",
    "    reward: float\n",
    "    min_ram: float\n",
    "    model_config = ConfigDict(frozen=True)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.model_id, self.reward, self.min_ram))\n",
    "\n",
    "\n",
    "class ModelZoo:\n",
    "    models_configs: ClassVar[list[ModelConfig]] = [\n",
    "        ModelConfig(model_id=\"casperhansen/mistral-nemo-instruct-2407-awq\", reward=0.1, min_ram=24),\n",
    "        ModelConfig(model_id=\"casperhansen/llama-3-8b-instruct-awq\", reward=0.1, min_ram=24),\n",
    "        ModelConfig(model_id=\"casperhansen/llama-3-70b-instruct-awq\", reward=0.8, min_ram=70),\n",
    "    ]\n",
    "\n",
    "    @classmethod\n",
    "    def get_all_models(cls) -> list[str]:\n",
    "        return [model.model_id for model in cls.models_configs]\n",
    "\n",
    "    @classmethod\n",
    "    def get_random(cls, max_ram: float = np.inf) -> ModelConfig:\n",
    "        models = [model for model in cls.models_configs if model.min_ram <= max_ram]\n",
    "        return np.random.choice(models)\n",
    "\n",
    "    @classmethod\n",
    "    def get_model_by_id(cls, model_id: str) -> ModelConfig:\n",
    "        return [model for model in cls.models_configs if model.model_id == model_id][0]\n",
    "\n",
    "\n",
    "class ModelManager(BaseModel):\n",
    "    always_active_models: list[ModelConfig] = []\n",
    "    total_ram: float = 40.0\n",
    "    active_models: dict[ModelConfig, vllm.LLM] = {}\n",
    "    used_ram: float = 0.0\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def load_always_active_models(self) -> \"ModelManager\":\n",
    "        for model_config in self.always_active_models:\n",
    "            self.load_model(model_config)\n",
    "        return self\n",
    "\n",
    "    def load_model(self, model_config: ModelConfig, force: bool = True):\n",
    "        # if force loading is enabled, unload models until there is enough RAM\n",
    "        if force:\n",
    "            for active_model in self.active_models.keys():\n",
    "                if active_model in self.always_active_models:\n",
    "                    continue\n",
    "                if self.used_ram + model_config.min_ram > self.total_ram or GPUInfo.free_memory < model_config.min_ram:\n",
    "                    logger.debug(f\"Unloading {active_model.model_id} to make room for {model_config.model_id}\")\n",
    "                    self.unload_model(active_model)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        if self.used_ram + model_config.min_ram > self.total_ram or GPUInfo.free_memory < model_config.min_ram:\n",
    "            raise MemoryError(\n",
    "                f\"\"\"Not enough RAM to load model {model_config.model_id}. \n",
    "                    Required: {model_config.min_ram} GB\n",
    "                    Available in Model Manager: {self.total_ram - self.used_ram} GB\n",
    "                    Available in GPU: {GPUInfo.free_memory} GB\"\"\"\n",
    "            )\n",
    "\n",
    "        if model_config in self.active_models.keys():\n",
    "            print(f\"Model {model_config.model_id} is already loaded.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            model = vllm.LLM(model_config.model_id, max_model_len=8_000)\n",
    "            self.active_models[model_config] = model\n",
    "            self.used_ram += model_config.min_ram\n",
    "            logger.info(f\"Model {model_config.model_id} loaded. Current used RAM: {self.used_ram} GB\")\n",
    "\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to load model {model_config.model_id}. Error: {str(e)}\")\n",
    "\n",
    "    def unload_model(self, model_config: ModelConfig):\n",
    "        if model_config not in self.active_models:\n",
    "            logger.warning(\"Couldn't find model to unload.\")\n",
    "            return\n",
    "        import gc\n",
    "\n",
    "        destroy_model_parallel()\n",
    "        try:\n",
    "            del self.active_models[model_config].llm_engine.model_executor.driver_worker\n",
    "            del self.active_models[model_config]\n",
    "        except:\n",
    "            pass\n",
    "        gc.collect()\n",
    "        self.used_ram -= model_config.min_ram\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def get_or_load(self, model_id: str) -> vllm.LLM:\n",
    "        model_config = ModelZoo.get_model_by_id(model_id)\n",
    "        if model_config not in self.active_models:\n",
    "            self.load_model(model_config)\n",
    "        return self.active_models[model_config]\n",
    "\n",
    "\n",
    "# keep model used for validation always active\n",
    "\n",
    "\n",
    "import asyncio\n",
    "import random\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "\n",
    "class AsyncModelScheduler(BaseModel):\n",
    "    model_manager: ModelManager\n",
    "    interval: int = 10  # Minimum time in seconds for a model to stay active\n",
    "    running: bool = False\n",
    "\n",
    "    async def start(self):\n",
    "        self.running = True\n",
    "        await self.run_scheduler()\n",
    "\n",
    "    async def stop(self):\n",
    "        self.running = False\n",
    "\n",
    "    async def run_scheduler(self):\n",
    "        while self.running:\n",
    "            selected_model = ModelZoo.get_random(max_ram=self.model_manager.total_ram)\n",
    "            logger.info(f\"Loading model {selected_model.model_id} for {self.interval} seconds.\")\n",
    "            \n",
    "            if selected_model in self.model_manager.active_models:\n",
    "                logger.info(f\"Model {selected_model.model_id} is already loaded.\")\n",
    "                return\n",
    "            # Load the selected model\n",
    "            await self.load_model_async(selected_model)\n",
    "            \n",
    "            # Keep the model loaded for the specified time interval\n",
    "            await asyncio.sleep(self.interval)\n",
    "            \n",
    "            # After the interval, unload the model if it is not in always_active_models\n",
    "            if selected_model not in self.model_manager.always_active_models:\n",
    "                logger.info(f\"Unloading model {selected_model.model_id} after {self.interval} seconds.\")\n",
    "                self.model_manager.unload_model(selected_model)\n",
    "\n",
    "        logger.info(\"Model scheduler stopped.\")\n",
    "\n",
    "    async def load_model_async(self, model_config: ModelConfig):\n",
    "        loop = asyncio.get_event_loop()\n",
    "        await loop.run_in_executor(None, self.model_manager.load_model, model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_manager = ModelManager(always_active_models=[ModelZoo.get_model_by_id(model_id=ModelZoo.models_configs[0].model_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "model_manager = ModelManager()\n",
    "    \n",
    "model_scheduler = AsyncModelScheduler(model_manager=model_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.normal(size=10)\n",
    "successes = np.sum([a < max_bound] and [a > min_bound])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:20:32.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/llama-3-8b-instruct-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:20:32 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:20:32 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/llama-3-8b-instruct-awq', speculative_config=None, tokenizer='casperhansen/llama-3-8b-instruct-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/llama-3-8b-instruct-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:20:33 model_runner.py:680] Starting to load model casperhansen/llama-3-8b-instruct-awq...\n",
      "INFO 08-14 12:20:33 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  5.47it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.50it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.68it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:20:36 model_runner.py:692] Loading model weights took 5.3453 GB\n",
      "INFO 08-14 12:20:38 gpu_executor.py:102] # GPU blocks: 16735, # CPU blocks: 2048\n",
      "INFO 08-14 12:20:40 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:20:40 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:20:56 model_runner.py:1181] Graph capturing finished in 16 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:20:56.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/llama-3-8b-instruct-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:21:06.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/llama-3-8b-instruct-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:21:06.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:21:07 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:21:07 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:21:07 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:21:08 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.35it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.11it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:21:11 model_runner.py:692] Loading model weights took 8.0802 GB\n",
      "INFO 08-14 12:21:13 gpu_executor.py:102] # GPU blocks: 12277, # CPU blocks: 1638\n",
      "INFO 08-14 12:21:15 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:21:15 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:21:30 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:21:30.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:21:40.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:21:40.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:21:40 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:21:40 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:21:41 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:21:41 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.72it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.34it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:21:44 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:21:46 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:21:47 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:21:47 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:22:00 model_runner.py:1181] Graph capturing finished in 13 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:22:00.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:22:10.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:22:11.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:22:11 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:22:11 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:22:11 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:22:11 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.61it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.25it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:22:14 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:22:17 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:22:17 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:22:17 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:22:31 model_runner.py:1181] Graph capturing finished in 13 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:22:31.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:22:41.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:22:41.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:22:41 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:22:41 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:22:42 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:22:42 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.77it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.30it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.35it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:22:45 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:22:47 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:22:48 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:22:48 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:23:03 model_runner.py:1181] Graph capturing finished in 15 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:23:03.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:23:13.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:23:14.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:23:14 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:23:14 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:23:15 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:23:15 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.54it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:23:18 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:23:20 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:23:21 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:23:21 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:23:35 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:23:35.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:23:45.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:23:46.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:23:46 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:23:46 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:23:47 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:23:47 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.60it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.23it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:23:50 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:23:52 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:23:52 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:23:52 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:24:06 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:24:06.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:24:16.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:24:17.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:24:17 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:24:17 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:24:18 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:24:18 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.67it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.26it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:24:21 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:24:23 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:24:24 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:24:24 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:24:38 model_runner.py:1181] Graph capturing finished in 15 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:24:38.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:24:48.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:24:49.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:24:49 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:24:49 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:24:50 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:24:50 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.58it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:24:53 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:24:55 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:24:56 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:24:56 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:25:10 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:25:10.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:25:20.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:25:21.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:25:21 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:25:21 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:25:21 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:25:21 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.66it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.25it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:25:24 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:25:27 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:25:27 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:25:27 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:25:41 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:25:41.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:25:51.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:25:52.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:25:52 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:25:52 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:25:52 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:25:52 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.61it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.23it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:25:55 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:25:58 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:25:58 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:25:58 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:26:12 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:26:12.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:26:22.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:26:23.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:26:23 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:26:23 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:26:24 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:26:24 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.58it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.23it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:26:26 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:26:29 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:26:29 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:26:29 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:26:43 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:26:43.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:26:53.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:26:54.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:26:54 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:26:54 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:26:54 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:26:55 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.57it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.17it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:26:58 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:27:00 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:27:01 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:27:01 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:27:14 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:27:14.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:27:24.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:27:25.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:27:25 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:27:25 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:27:26 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:27:26 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.60it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:27:29 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:27:31 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:27:32 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:27:32 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:27:46 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:27:46.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:27:56.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:27:57.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:27:57 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:27:57 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:27:57 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:27:57 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.62it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.26it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:28:00 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:28:03 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:28:03 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:28:03 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:28:17 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:28:17.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:28:27.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:28:27.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:28:28 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:28:28 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:28:28 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:28:28 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.68it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.26it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.31it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:28:31 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:28:34 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:28:34 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:28:34 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:28:47 model_runner.py:1181] Graph capturing finished in 13 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:28:47.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:28:57.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:28:58.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:28:58 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:28:58 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:28:58 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:28:59 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.80it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.32it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.37it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:29:01 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:29:04 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:29:04 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:29:04 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:29:17 model_runner.py:1181] Graph capturing finished in 13 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:29:17.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:29:27.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:29:28.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:29:28 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:29:28 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:29:29 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:29:29 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.60it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.23it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:29:32 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:29:34 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:29:35 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:29:35 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:29:49 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:29:49.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:29:59.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:29:59.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:29:59 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:29:59 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:30:00 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:30:00 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.69it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.23it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:30:03 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:30:05 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:30:07 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:30:07 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:30:22 model_runner.py:1181] Graph capturing finished in 15 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:30:22.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:30:32.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:30:33.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:30:33 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:30:33 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:30:33 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:30:34 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.62it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.23it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:30:36 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:30:39 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:30:40 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:30:40 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:30:53 model_runner.py:1181] Graph capturing finished in 13 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:30:53.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:31:03.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:31:04.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:31:04 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:31:04 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:31:04 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:31:05 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.67it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.22it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.27it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:31:07 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:31:10 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:31:10 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:31:10 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:31:26 model_runner.py:1181] Graph capturing finished in 15 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:31:26.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:31:36.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:31:37.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:31:37 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:31:37 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:31:37 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:31:38 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.56it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.22it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:31:40 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:31:43 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:31:44 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:31:44 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:31:58 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:31:58.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:32:08.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:32:08.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:32:08 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:32:08 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:32:09 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:32:09 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.63it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.26it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:32:12 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:32:15 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:32:16 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:32:16 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:32:30 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:32:30.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:32:40.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:32:41.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:32:41 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:32:41 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:32:41 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:32:41 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.63it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.24it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:32:44 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:32:47 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:32:47 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:32:47 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:33:02 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:33:02.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:33:12.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:33:13.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:33:13 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:33:13 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:33:13 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:33:13 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.38it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.23it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:33:16 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:33:19 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:33:19 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:33:19 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:33:34 model_runner.py:1181] Graph capturing finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:33:34.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:33:44.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:33:44.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:33:45 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:33:45 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:33:45 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:33:45 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.57it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.24it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:33:48 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:33:51 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:33:51 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:33:51 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:34:04 model_runner.py:1181] Graph capturing finished in 13 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:34:04.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:34:14.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:34:15.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:34:15 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:34:15 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:34:16 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:34:16 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.61it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:34:19 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:34:22 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:34:22 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:34:22 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:34:35 model_runner.py:1181] Graph capturing finished in 13 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:34:35.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:34:45.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:34:46.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:34:46 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:34:46 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:34:47 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:34:47 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.60it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.24it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:34:50 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:34:52 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:34:53 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:34:53 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:35:06 model_runner.py:1181] Graph capturing finished in 13 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:35:06.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n",
      "\u001b[32m2024-08-14 12:35:16.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mUnloading model casperhansen/mistral-nemo-instruct-2407-awq after 10 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-14 12:35:16.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_scheduler\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mLoading model casperhansen/mistral-nemo-instruct-2407-awq for 10 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:35:17 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-14 12:35:17 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/mistral-nemo-instruct-2407-awq', speculative_config=None, tokenizer='casperhansen/mistral-nemo-instruct-2407-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/mistral-nemo-instruct-2407-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-14 12:35:17 model_runner.py:680] Starting to load model casperhansen/mistral-nemo-instruct-2407-awq...\n",
      "INFO 08-14 12:35:18 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.47it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.17it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 12:35:21 model_runner.py:692] Loading model weights took 7.8469 GB\n",
      "INFO 08-14 12:35:23 gpu_executor.py:102] # GPU blocks: 12365, # CPU blocks: 1638\n",
      "INFO 08-14 12:35:24 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 12:35:24 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-14 12:35:36 model_runner.py:1181] Graph capturing finished in 12 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 12:35:36.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mModel casperhansen/mistral-nemo-instruct-2407-awq loaded. Current used RAM: 24.0 GB\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 # Start the scheduler asynchronously</span>                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 asyncio.run(model_scheduler.start())                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">nest_as</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">yncio.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">30</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 27 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loop.set_debug(debug)                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 28 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>task = asyncio.ensure_future(main)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 29 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 30 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loop.run_until_complete(task)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 31 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 32 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> task.done():                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 33 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>task.cancel()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">nest_as</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">yncio.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">92</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run_until_complete</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 89 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> f <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> future:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 90 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>f._log_destroy_pending = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 91 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> f.done():                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 92 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._run_once()                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 93 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._stopping:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 94 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">break</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> f.done():                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">nest_as</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">yncio.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">115</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_run_once</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">min</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">max</span>(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>scheduled[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]._when - <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.time(), <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>), <span style=\"color: #0000ff; text-decoration-color: #0000ff\">86400</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> scheduled                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>)                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>115 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>event_list = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._selector.select(timeout)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._process_events(event_list)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>end_time = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.time() + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._clock_resolution                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/lib/python3.10/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">selectors.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">469</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">select</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">466 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">467 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>ready = []                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">468 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>469 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>fd_event_list = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._selector.poll(timeout, max_ev)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">470 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">InterruptedError</span>:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">471 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> ready                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">472 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> fd, event <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> fd_event_list:                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0m\u001b[2m# Start the scheduler asynchronously\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2 asyncio.run(model_scheduler.start())                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/\u001b[0m\u001b[1;33mnest_as\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33myncio.py\u001b[0m:\u001b[94m30\u001b[0m in \u001b[92mrun\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 27 \u001b[0m\u001b[2m│   │   \u001b[0mloop.set_debug(debug)                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 28 \u001b[0m\u001b[2m│   │   \u001b[0mtask = asyncio.ensure_future(main)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 29 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 30 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m loop.run_until_complete(task)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 31 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfinally\u001b[0m:                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 32 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m task.done():                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 33 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtask.cancel()                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/\u001b[0m\u001b[1;33mnest_as\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33myncio.py\u001b[0m:\u001b[94m92\u001b[0m in \u001b[92mrun_until_complete\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 89 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m f \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m future:                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 90 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mf._log_destroy_pending = \u001b[94mFalse\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[95mnot\u001b[0m f.done():                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 92 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._run_once()                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._stopping:                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m f.done():                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/\u001b[0m\u001b[1;33mnest_as\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33myncio.py\u001b[0m:\u001b[94m115\u001b[0m in \u001b[92m_run_once\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m \u001b[96mmin\u001b[0m(\u001b[96mmax\u001b[0m(                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mscheduled[\u001b[94m0\u001b[0m]._when - \u001b[96mself\u001b[0m.time(), \u001b[94m0\u001b[0m), \u001b[94m86400\u001b[0m) \u001b[94mif\u001b[0m scheduled                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m \u001b[94mNone\u001b[0m)                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m115 \u001b[2m│   │   \u001b[0mevent_list = \u001b[96mself\u001b[0m._selector.select(timeout)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._process_events(event_list)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m\u001b[2m│   │   \u001b[0mend_time = \u001b[96mself\u001b[0m.time() + \u001b[96mself\u001b[0m._clock_resolution                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/lib/python3.10/\u001b[0m\u001b[1;33mselectors.py\u001b[0m:\u001b[94m469\u001b[0m in \u001b[92mselect\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m466 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m467 \u001b[0m\u001b[2m│   │   │   \u001b[0mready = []                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m468 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m469 \u001b[2m│   │   │   │   \u001b[0mfd_event_list = \u001b[96mself\u001b[0m._selector.poll(timeout, max_ev)                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m470 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mInterruptedError\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m471 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m ready                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m472 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m fd, event \u001b[95min\u001b[0m fd_event_list:                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Start the scheduler asynchronously\n",
    "asyncio.run(model_scheduler.start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it, est. speed input: 3.75 toks/s, output: 62.46 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=2, prompt='Hello, my name is', prompt_token_ids=[1, 22177, 1044, 2036, 2564, 1395], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\" [Your Name], and I am a [Your Age]-year-old [Your Gender] from [Your City/State/Country]. I am here to learn about [Your Interest or Goal]. I would like to ask you some questions about [Your Topic of Interest] if you don't mind.\\n\\n في: Hi, my name is [Your Name], and I am a [Your Age]-year-old [Your Gender] from [Your City/State/Country]. I am here to learn about\", token_ids=(1766, 16994, 9878, 3605, 1321, 1362, 1855, 1261, 1766, 16994, 21191, 26118, 26098, 15962, 1766, 16994, 59944, 1093, 1562, 1766, 16994, 6308, 1047, 3906, 14787, 15543, 3077, 1362, 1855, 3226, 1317, 8178, 2314, 1766, 16994, 24789, 1505, 57148, 3077, 1362, 2168, 2479, 1317, 4237, 1636, 2269, 8352, 2314, 1766, 16994, 96620, 1307, 24789, 1093, 1693, 1636, 2607, 2405, 5759, 1338, 1819, 1058, 24665, 1044, 2036, 2564, 1395, 1766, 16994, 9878, 3605, 1321, 1362, 1855, 1261, 1766, 16994, 21191, 26118, 26098, 15962, 1766, 16994, 59944, 1093, 1562, 1766, 16994, 6308, 1047, 3906, 14787, 15543, 3077, 1362, 1855, 3226, 1317, 8178, 2314), cumulative_logprob=-25.88084539026022, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1723636760.0991132, last_token_time=1723636760.0991132, first_scheduled_time=1723636760.101751, first_token_time=1723636760.1244695, time_in_queue=0.0026378631591796875, finished_time=1723636761.7024746), lora_request=None)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(seed=43, max_tokens=100, temperature=0.5, top_p=0.9, top_k=50)\n",
    "list(model_manager.active_models.values())[0].generate(\"Hello, my name is\", sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 model_manager.load_model(ModelZoo.get_random())                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load_model</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">72</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 69 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">break</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 70 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 71 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.used_ram + model_config.min_ram &gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.total_ram <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> GPUInfo.free_memory    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 72 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">MemoryError</span>(                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 73 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"\"\"Not enough RAM to load model {</span>model_config.model_id<span style=\"color: #808000; text-decoration-color: #808000\">}. </span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 74 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">Required: {</span>model_config.min_ram<span style=\"color: #808000; text-decoration-color: #808000\">} GB</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 75 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">Available in Model Manager: {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.total_ram<span style=\"color: #808080; text-decoration-color: #808080\"> </span>-<span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.used_ram<span style=\"color: #808000; text-decoration-color: #808000\">} GB</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">MemoryError: </span>Not enough RAM to load model casperhansen/mistral-nemo-instruct-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2407</span>-awq. \n",
       "                    Required: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.0</span> GB\n",
       "                    Available in Model Manager: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.0</span> GB\n",
       "                    Available in GPU: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.622802734375</span> GB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 model_manager.load_model(ModelZoo.get_random())                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mload_model\u001b[0m:\u001b[94m72\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 69 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 70 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 71 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.used_ram + model_config.min_ram > \u001b[96mself\u001b[0m.total_ram \u001b[95mor\u001b[0m GPUInfo.free_memory    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 72 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mMemoryError\u001b[0m(                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 73 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\"\"\u001b[0m\u001b[33mNot enough RAM to load model \u001b[0m\u001b[33m{\u001b[0mmodel_config.model_id\u001b[33m}\u001b[0m\u001b[33m. \u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 74 \u001b[0m\u001b[2;33m│   │   │   │   │   \u001b[0m\u001b[33mRequired: \u001b[0m\u001b[33m{\u001b[0mmodel_config.min_ram\u001b[33m}\u001b[0m\u001b[33m GB\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 75 \u001b[0m\u001b[2;33m│   │   │   │   │   \u001b[0m\u001b[33mAvailable in Model Manager: \u001b[0m\u001b[33m{\u001b[0m\u001b[96mself\u001b[0m.total_ram\u001b[90m \u001b[0m-\u001b[90m \u001b[0m\u001b[96mself\u001b[0m.used_ram\u001b[33m}\u001b[0m\u001b[33m GB\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mMemoryError: \u001b[0mNot enough RAM to load model casperhansen/mistral-nemo-instruct-\u001b[1;36m2407\u001b[0m-awq. \n",
       "                    Required: \u001b[1;36m24.0\u001b[0m GB\n",
       "                    Available in Model Manager: \u001b[1;36m16.0\u001b[0m GB\n",
       "                    Available in GPU: \u001b[1;36m4.622802734375\u001b[0m GB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_manager.load_model(ModelZoo.get_random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.999755859375, 44.3516845703125)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "free, total = torch.cuda.mem_get_info(0) \n",
    "free / (1024**3), total / (1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_manager.active_models[model_config].llm_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm is a vllm.LLM object\n",
    "import gc\n",
    "import torch\n",
    "from vllm.model_executor.parallel_utils.parallel_state import destroy_model_parallel\n",
    "\n",
    "destroy_model_parallel()\n",
    "#del a vllm.executor.ray_gpu_executor.RayGPUExecutor object\n",
    "del llm.llm_engine.model_executor\n",
    "del llm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(relogin=True, key=\"ae29a588c238d0e168d620e0b18a5e29e283935a\")\n",
    "wandb.init(\n",
    "    project=\"validators\",\n",
    "    entity=\"felix-quinque-macrocosmos-ai\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "WANDB = wandb.init(\n",
    "        project=\"validator\",\n",
    "        entity=\"felix-quinque-macrocosmos\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bittensor as bt\n",
    "\n",
    "wallet = bt.wallet(name=\"dalkfjsl\", hotkey=None)\n",
    "metagraph = bt.metagraph(netuid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Version:\n",
    "    \"\"\"Same as packaging.version, but also supports comparison to strings\"\"\"\n",
    "\n",
    "    def __init__(self, version: str = \"1.2.3\"):\n",
    "        self.version: str = version\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.version}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.version}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        other = other.version if isinstance(other, Version) else other\n",
    "        return self.version == other\n",
    "\n",
    "    def __le__(self, other):\n",
    "        other = other.version if isinstance(other, Version) else other\n",
    "        return True if all([v <= o for v, o in zip(self.version.split(\".\"), other.split(\".\"))]) else False\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        other = other.version if isinstance(other, Version) else other\n",
    "        return True if self <= other and self != other else False\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        other = other.version if isinstance(other, Version) else other\n",
    "        return True if not (self < other) else False\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        other = other.version if isinstance(other, Version) else other\n",
    "        return True if not (self <= other) else False\n",
    "\n",
    "\n",
    "\n",
    "Version(\"1.2.3\") < \"1.2.2.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_split = \"1.2.3\".split(\".\")\n",
    "(1000 * int(version_split[0])) + (10 * int(version_split[1])) + (1 * int(version_split[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "i = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(10):\n",
    "    time.sleep(0.1)\n",
    "    print(f\"{-start_time+(start_time := time.time())} seconds have passed\")\n",
    "    # print(f\"{(start_time := time.time())-start_time} seconds have passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Foo():\n",
    "    def __init__(self):\n",
    "        self.__bar = 42\n",
    "\n",
    "foo = Foo()\n",
    "foo.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bittensor as bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metagraph = bt.metagraph(netuid=61, network=\"test\", sync=True, lite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metagraph.axons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x = np.reshape(np.random.random(size=10), (1,-1))\n",
    "y = np.reshape(np.random.random(size=10), (-1,1))\n",
    "\n",
    "float(x@y), x@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from prompting.tasks.date_qa import DateQuestionAnsweringTask\n",
    "from prompting.tasks.base_task import BaseTask\n",
    "from prompting.rewards.reward import BaseRewardModel\n",
    "from prompting.tasks.summarization import SummarizationTask, SummarizationRewardConfig\n",
    "from prompting.tasks.qa import QuestionAnsweringTask, QARewardConfig\n",
    "\n",
    "from prompting.datasets.wiki import WikiDataset\n",
    "from prompting.datasets.base import BaseDataset\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "import random\n",
    "from typing import ClassVar\n",
    "import bittensor as bt\n",
    "\n",
    "\n",
    "class TaskConfig(BaseModel):\n",
    "    task: BaseTask.__class__\n",
    "    probability: float\n",
    "    datasets: list[BaseDataset.__class__]\n",
    "    reward_model: BaseRewardModel.__class__\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "\n",
    "class TaskRegistry(BaseModel):\n",
    "    tasks: ClassVar[list[TaskConfig]] = [\n",
    "        TaskConfig(task=QuestionAnsweringTask, probability=0.6, datasets=[WikiDataset], reward_model=QARewardConfig),\n",
    "        TaskConfig(\n",
    "            task=SummarizationTask, probability=0.4, datasets=[WikiDataset], reward_model=SummarizationRewardConfig\n",
    "        ),\n",
    "        # TaskConfig(task=DateQuestionAnsweringTask, probability=0.2, datasets=[WikiDateDataset])\n",
    "    ]\n",
    "\n",
    "    @classmethod\n",
    "    def random(self) -> TaskConfig:\n",
    "        probabilities = [task.probability for task in self.tasks]\n",
    "        selected_task = random.choices(self.tasks, probabilities)[0]\n",
    "        return selected_task\n",
    "\n",
    "    @classmethod\n",
    "    def get_task_datasets(self, task: BaseTask.__class__):\n",
    "        try:\n",
    "            return [t.datasets for t in self.tasks if task is t.task][0]\n",
    "        except Exception:\n",
    "            bt.logging.error(\"Tried accessing non-registered task\")\n",
    "            return []\n",
    "\n",
    "    @classmethod\n",
    "    def get_random_task_dataset(self, task: BaseTask.__class__) -> BaseDataset.__class__:\n",
    "        return random.choice(self.get_task_datasets(task))\n",
    "\n",
    "    @classmethod\n",
    "    def get_task_reward(self, task: BaseTask) -> BaseRewardModel:\n",
    "        try:\n",
    "            return [t.reward_model for t in self.tasks if task is t.task][0]\n",
    "        except Exception:\n",
    "            bt.logging.error(\"Tried accessing non-registered task\")\n",
    "            return []\n",
    "\n",
    "    @classmethod\n",
    "    def create_random_task(self, llm_pipeline) -> BaseTask:\n",
    "        task_config = self.random()\n",
    "        dataset = self.get_random_task_dataset(task_config.task)\n",
    "        return task_config.task(\n",
    "            llm_pipeline=llm_pipeline, context=dataset().next(), reward_config=task_config.reward_model()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaskRegistry().get_task_reward(QuestionAnsweringTask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompting-fb5sw-i7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
