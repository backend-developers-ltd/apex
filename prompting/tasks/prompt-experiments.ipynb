{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompting.datasets.wiki import WikiDataset\n",
    "dataset = WikiDataset()\n",
    "\n",
    "samples = []\n",
    "for i in range(2):\n",
    "    samples.append(dataset.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in samples:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the analysis\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_length_index(answers: List[str], correct: int) -> int:\n",
    "    \"\"\"Returns the index of the correct answer once the answers are sorted by string length.\"\"\"\n",
    "    assert len(answers) == 4\n",
    "    sorted_by_length = sorted(answers, key=len)\n",
    "    return sorted_by_length.index(answers[correct])\n",
    "\n",
    "\n",
    "def plot_results(prompt_name, questions, answers):\n",
    "    answer_map = {\n",
    "        \"A\": 0,\n",
    "        \"B\": 1,\n",
    "        \"C\": 2,\n",
    "        \"D\": 3\n",
    "    }\n",
    "    \n",
    "    # Convert the answer into an index.\n",
    "    correct_answers = [answer_map[a] for a in answers]\n",
    "\n",
    "    r = \"\\[Input Question\\]\\s+(.*)\\s+A\\. (.*)\\s+B\\. (.*)\\s+C\\. (.*)\\s+D\\. (.*)\"\n",
    "    regex_trim = \"\\[Input Question\\]\\s+([\\s\\S]*)Answer:.*\"\n",
    "\n",
    "    length_counts = defaultdict(list)\n",
    "\n",
    "    for question, correct in zip(questions, correct_answers):\n",
    "        match = re.search(r, question)\n",
    "        if not match:\n",
    "            print(f\"WTF: {question}\")\n",
    "            continue\n",
    "        \n",
    "        trimmed_qa = re.search(regex_trim, question).groups()[0]\n",
    "        four_answers = match.groups()[1:]\n",
    "        length_counts[get_length_index(four_answers, correct)].append((trimmed_qa, correct))\n",
    "\n",
    "    x_and_y = [(index, len(items)) for index, items in length_counts.items()]\n",
    "    x, y = zip(*x_and_y)\n",
    "\n",
    "    plt.bar(x, y) \n",
    "    plt.xlabel(\"Index of correct ans by answer length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(prompt_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompting import settings\n",
    "import prompting\n",
    "settings.settings = settings.Settings(mode=\"validator\")\n",
    "settings = settings.settings\n",
    "from prompting.tasks.multi_choice import MultiChoiceTask\n",
    "\n",
    "def create_multi_choice_question(sample, system_prompt, user_prompt):\n",
    "    task = MultiChoiceTask()\n",
    "    _ = task.make_query(sample, system_prompt, user_prompt)\n",
    "    return (task.query, task.reference)\n",
    "\n",
    "USER_PROMPT_WITH_LENGTH_PROMPT = \"\"\"\\\n",
    "Create a multiple choice quiz based on the following context source from {source} about {title}. All answers must be of similar length, tone and style.\n",
    "\n",
    "[Input Context]\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_WITH_LENGTH_PROMPT = \"\"\"You are a multiple choice quiz-generating expert.\n",
    "Based on the input context, you must generate the question, exactly 4 possible answers (A, B, C, D), and the correct answer letter.\n",
    "All answers must be of similar length, tone and style.\n",
    "\n",
    "[Example 1]\n",
    "{\n",
    "    \"question\": \"What is the capital of Texas?\",\n",
    "    \"A\": \"Paris\",\n",
    "    \"B\": \"London\",\n",
    "    \"C\": \"Austin\",\n",
    "    \"D\": \"Houston\",\n",
    "    \"answer\": \"C\"\n",
    "}\n",
    "\n",
    "[Example 2]\n",
    "{\n",
    "    \"question\": \"Which of the following best describes the primary driving force behind protein folding?\",\n",
    "    \"A\": \"Covalent bond formation between amino acids\",\n",
    "    \"B\": \"Hydrophobic interactions between nonpolar side chains\",\n",
    "    \"C\": \"Hydrogen bonds between the protein backbone and side chains\",\n",
    "    \"D\": \"Ionic interactions between charged side chains\",\n",
    "    \"answer\": \"B\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_MORE_EXAMPLES = \"\"\"You are a multiple choice quiz-generating expert.\n",
    "Based on the input context, you must generate the question, exactly 4 possible answers (A, B, C, D), and the correct answer letter.\n",
    "All answers must be of similar length, tone and style.\n",
    "\n",
    "[Example 1]\n",
    "{\n",
    "    \"question\": \"Which of the following is not an element of the redistribution-with-growth policy approach?\",\n",
    "    \"A\": \"minimum wage legislation\",\n",
    "    \"B\": \"land reform\",\n",
    "    \"C\": \"progressive taxation\",\n",
    "    \"D\": \"increased access to education\",\n",
    "    \"answer\": \"A\"\n",
    "}\n",
    "\n",
    "[Example 2]\n",
    "{\n",
    "    \"question\": \"Which of the following best describes the primary driving force behind protein folding?\",\n",
    "    \"A\": \"Covalent bond formation between amino acids\",\n",
    "    \"B\": \"Hydrophobic interactions between nonpolar side chains\",\n",
    "    \"C\": \"Hydrogen bonds between the protein backbone and side chains\",\n",
    "    \"D\": \"Ionic interactions between charged side chains\",\n",
    "    \"answer\": \"B\"\n",
    "}\n",
    "\n",
    "[Example 3]\n",
    "{\n",
    "    \"question\": \"What is the capital of Texas?\",\n",
    "    \"A\": \"Paris\",\n",
    "    \"B\": \"London\",\n",
    "    \"C\": \"Austin\",\n",
    "    \"D\": \"Houston\",\n",
    "    \"answer\": \"C\"\n",
    "}\n",
    "\n",
    "[Example 4]\n",
    "{\n",
    "    \"question\": \"What interior discipline must be adopted to achieve spiritual liberation within Sikhism?\",\n",
    "    \"A\": \"Remembering the Divine Name\",\n",
    "    \"B\": \"Meditating on the sacred hymns\",\n",
    "    \"C\": \"Remembering that death is inevitable\",\n",
    "    \"D\": \"Meditating on the goodness of the created world\",\n",
    "    \"answer\": \"A\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "prompts = {\n",
    "    \"default\": (prompting.tasks.multi_choice.QUERY_SYSTEM_PROMPT, prompting.tasks.multi_choice.MultiChoiceTask.QUERY_USER_PROMPT),\n",
    "    \"length_instructions\": (prompting.tasks.multi_choice.QUERY_SYSTEM_PROMPT, USER_PROMPT_WITH_LENGTH_PROMPT),\n",
    "    \"more_examples\": (SYSTEM_PROMPT_MORE_EXAMPLES, prompting.tasks.multi_choice.MultiChoiceTask.QUERY_USER_PROMPT),\n",
    "    \"more_examples_and_length\": (SYSTEM_PROMPT_MORE_EXAMPLES, USER_PROMPT_WITH_LENGTH_PROMPT),\n",
    "}\n",
    "\n",
    "for name, (system_prompt, user_prompt) in prompts.items():\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for sample in samples:\n",
    "        try:\n",
    "            question, answer = create_multi_choice_question(sample, system_prompt, user_prompt)\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create sample for {name}\")      \n",
    "            \n",
    "    plot_results(name, questions, answers)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
